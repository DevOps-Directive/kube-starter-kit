---
title: 'Deploy Kubernetes Baseline'
description: 'Bootstrap ArgoCD and deploy infrastructure components'
icon: 'dharmachakra'
---

## Overview

With EKS clusters deployed, the next step is to bootstrap ArgoCD and deploy the Kubernetes baseline components. ArgoCD uses an app-of-apps pattern to manage all cluster resources declaratively.

<Note>
Before proceeding, ensure you have completed [Deploy Infrastructure](/usage/getting-started/06-deploy-infrastructure) and can access the cluster with kubectl.
</Note>

## Architecture

The kit uses a three-tier app-of-apps pattern:

```
argocd-app-of-apps (root)
├── argocd                    → Self-manages ArgoCD installation
├── infrastructure-app-of-apps → Manages infrastructure components
│   ├── cert-manager
│   ├── ingress-nginx
│   ├── external-secrets
│   ├── external-dns
│   ├── karpenter
│   ├── cloudnative-pg
│   ├── signoz-k8s-infra
│   └── reloader
└── services-app-of-apps      → Manages application deployments
    ├── go-backend
    └── go-backend-helm
```

Each ArgoCD Application points to rendered manifests in `kubernetes/rendered/{cluster}/`.

## Update Repository URLs

Before deploying, update the Git repository URLs to point to your fork:

<Steps>
  <Step title="Update ArgoCD application sources">
    The Application manifests reference the Git repository. Update the `repoURL` in the source templates:

    **`kubernetes/src/argocd/argocd/templates/Application.argocd-app-of-apps.yaml`:**

    ```yaml
    spec:
      source:
        repoURL: git@github.com:YOUR-ORG/kube-starter-kit.git  # Update this
        targetRevision: main
        path: kubernetes/rendered/{{ .Values.cluster }}/argocd/argocd
    ```

    Update this URL in all Application templates:
    - `kubernetes/src/argocd/argocd/templates/Application.*.yaml`
    - `kubernetes/src/argocd/infrastructure/templates/Application.GENERATOR.yaml`
    - `kubernetes/src/argocd/services/templates/Application.GENERATOR.yaml`
  </Step>

  <Step title="Render updated manifests">
    After updating the source files, render the manifests for your cluster:

    ```bash
    cd kubernetes
    mise run render
    ```

    This renders all charts for all environments (staging, production).
  </Step>

  <Step title="Commit the changes">
    ```bash
    git add .
    git commit -m "chore: update ArgoCD repository URLs for fork"
    git push
    ```
  </Step>
</Steps>

## Store GitHub Deploy Key in AWS Secrets Manager

ArgoCD needs SSH access to clone your private repository. The kit uses External Secrets to fetch the deploy key from AWS Secrets Manager.

<Steps>
  <Step title="Generate a deploy key">
    ```bash
    ssh-keygen -t ed25519 -C "argocd-deploy-key" -f argocd-deploy-key -N ""
    ```

    This creates `argocd-deploy-key` (private) and `argocd-deploy-key.pub` (public).
  </Step>

  <Step title="Add the public key to GitHub">
    1. Go to your repository **Settings → Deploy keys**
    2. Click **Add deploy key**
    3. Paste the contents of `argocd-deploy-key.pub`
    4. Enable **Allow write access** if ArgoCD will push (optional)
  </Step>

  <Step title="Store the private key in AWS Secrets Manager">
    ```bash
    aws secretsmanager create-secret \
      --name "staging/argocd/repo-kube-starter-kit" \
      --secret-string "$(cat argocd-deploy-key)" \
      --region us-east-2
    ```

    Create secrets for each ArgoCD project (argocd, infrastructure, services):
    ```bash
    # The kit expects these secret paths:
    # staging/argocd/repo-argocd-kube-starter-kit
    # staging/argocd/repo-infrastructure-kube-starter-kit
    # staging/argocd/repo-services-kube-starter-kit
    ```

    <Tip>
    You can use the same deploy key for all three, or create separate keys for finer access control.
    </Tip>
  </Step>

  <Step title="Clean up local key files">
    ```bash
    rm argocd-deploy-key argocd-deploy-key.pub
    ```
  </Step>
</Steps>

## Bootstrap ArgoCD

<Steps>
  <Step title="Install ArgoCD">
    The EKS Terraform module creates the `argocd` namespace. Install ArgoCD using Helm:

    ```bash
    helm repo add argo https://argoproj.github.io/argo-helm
    helm repo update

    helm install argocd argo/argo-cd \
      --namespace argocd \
      --version 7.7.16 \
      --wait
    ```

    <Note>
    This installs a basic ArgoCD. Once the app-of-apps syncs, ArgoCD will manage its own configuration with your customizations (ingress, SSO, etc.).
    </Note>
  </Step>

  <Step title="Apply the root app-of-apps">
    Apply the rendered ArgoCD manifests to bootstrap the app-of-apps:

    ```bash
    kubectl apply -f kubernetes/rendered/staging/argocd/argocd/
    ```

    This creates:
    - Three AppProjects (argocd, infrastructure, services)
    - ExternalSecrets for GitHub deploy keys
    - The root `argocd-app-of-apps` Application
  </Step>

  <Step title="Wait for External Secrets to sync">
    The ExternalSecrets need to fetch the deploy keys before ArgoCD can clone the repository:

    ```bash
    kubectl get externalsecrets -n argocd
    kubectl get secrets -n argocd | grep repo-
    ```

    Wait until all ExternalSecrets show `SecretSynced` status.
  </Step>

  <Step title="Verify ArgoCD can sync">
    Check the ArgoCD Applications:

    ```bash
    kubectl get applications -n argocd
    ```

    You should see:
    - `argocd-app-of-apps` - Synced
    - `infrastructure-app-of-apps` - Synced
    - `services-app-of-apps` - Synced
  </Step>
</Steps>

## Access ArgoCD UI

<Steps>
  <Step title="Port-forward to ArgoCD">
    ```bash
    kubectl port-forward svc/argocd-server -n argocd 8080:443
    ```

    Open https://localhost:8080 in your browser.
  </Step>

  <Step title="Get the admin password">
    ```bash
    kubectl get secret argocd-initial-admin-secret -n argocd \
      -o jsonpath="{.data.password}" | base64 -d
    ```

    Login with username `admin` and the retrieved password.
  </Step>

  <Step title="Enable ingress (optional)">
    Once the infrastructure components deploy, ArgoCD will be accessible via ingress at `argocd.staging.yourdomain.com` (configured in `kubernetes/src/infrastructure/argocd/values.staging.yaml`).
  </Step>
</Steps>

## Infrastructure Components

Once ArgoCD syncs, the following infrastructure components deploy automatically:

| Component | Purpose |
|-----------|---------|
| **argocd** | Self-managed ArgoCD with ingress and GitHub OAuth |
| **cert-manager** | TLS certificate automation with Let's Encrypt |
| **ingress-nginx** | Ingress controller for routing external traffic |
| **external-secrets** | Syncs secrets from AWS Secrets Manager |
| **external-dns** | Automatic DNS record management in Route53 |
| **karpenter** | Dynamic node provisioning and autoscaling |
| **cloudnative-pg** | PostgreSQL operator for in-cluster databases |
| **signoz-k8s-infra** | OpenTelemetry collectors for observability |
| **reloader** | Restarts pods when ConfigMaps/Secrets change |
| **ebs-csi-driver-resources** | StorageClass for encrypted EBS volumes |

## Verify Deployment

<Steps>
  <Step title="Check all Applications are synced">
    ```bash
    kubectl get applications -n argocd
    ```

    All applications should show `Synced` and `Healthy` status.
  </Step>

  <Step title="Verify infrastructure pods">
    ```bash
    kubectl get pods -n cert-manager
    kubectl get pods -n ingress-nginx
    kubectl get pods -n external-secrets
    kubectl get pods -n karpenter
    ```

    All pods should be `Running`.
  </Step>

  <Step title="Check ingress is working">
    ```bash
    kubectl get ingress -A
    ```

    Ingresses should have an ADDRESS assigned (the load balancer DNS name).
  </Step>

  <Step title="Verify DNS records">
    Once external-dns is running, check Route53 for new records:

    ```bash
    aws route53 list-resource-record-sets \
      --hosted-zone-id <your-zone-id> \
      --query "ResourceRecordSets[?Type=='A' || Type=='CNAME']"
    ```
  </Step>
</Steps>

## Troubleshooting

### ExternalSecrets not syncing

1. Check the ClusterSecretStore is configured:
   ```bash
   kubectl get clustersecretstores
   kubectl describe clustersecretstore aws-secrets-manager
   ```

2. Verify Pod Identity is working:
   ```bash
   kubectl get pods -n external-secrets -o yaml | grep serviceAccountName
   kubectl describe serviceaccount -n external-secrets external-secrets
   ```

3. Check the secret exists in AWS Secrets Manager:
   ```bash
   aws secretsmanager get-secret-value \
     --secret-id staging/argocd/repo-argocd-kube-starter-kit \
     --region us-east-2
   ```

### ArgoCD can't clone repository

1. Check the repository secret exists:
   ```bash
   kubectl get secrets -n argocd | grep repo-
   ```

2. Verify the secret contains the SSH key:
   ```bash
   kubectl get secret repo-argocd-kube-starter-kit -n argocd -o yaml
   ```

3. Test SSH connectivity manually:
   ```bash
   ssh -T git@github.com -i /path/to/deploy-key
   ```

### Application stuck in "Progressing"

1. Check the Application events:
   ```bash
   kubectl describe application <app-name> -n argocd
   ```

2. Check for sync errors in ArgoCD UI

3. Manually sync with prune:
   ```bash
   argocd app sync <app-name> --prune
   ```

### Cert-manager certificates not issued

1. Check certificate status:
   ```bash
   kubectl get certificates -A
   kubectl describe certificate <cert-name> -n <namespace>
   ```

2. Check the ClusterIssuer:
   ```bash
   kubectl get clusterissuers
   kubectl describe clusterissuer letsencrypt-production
   ```

3. Check cert-manager logs:
   ```bash
   kubectl logs -n cert-manager -l app=cert-manager
   ```

## Enable/Disable Components

To enable or disable infrastructure components, edit `kubernetes/src/argocd/infrastructure/values.yaml`:

```yaml
applications:
  argocd:
    enabled: true
  cert-manager:
    enabled: true
  cloudnative-pg:
    enabled: true
  envoy-gateway:
    enabled: false  # Disabled by default
  # ... other components
```

After changing, render and push:

```bash
cd kubernetes
mise run render
git add . && git commit -m "chore: enable/disable components"
git push
```

ArgoCD will automatically sync the changes.

## Next Steps

With the Kubernetes baseline deployed, proceed to [Cluster Access](/usage/getting-started/08-cluster-access) to set up secure access to your clusters via bastion and SOCKS proxy.
